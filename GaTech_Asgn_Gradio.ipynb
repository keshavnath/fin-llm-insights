{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3-mqYKJQtOY"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0k53kLLlkoyD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#For raw data\n",
        "!pip install -U sec-edgar-downloader\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "# #For embeddings\n",
        "!pip install tiktoken\n",
        "!pip install chromadb\n",
        "\n",
        "# #For LLM Insights\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOqyKj5pQu-n"
      },
      "source": [
        "# Raw Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "p2jlcLN3lgr0"
      },
      "outputs": [],
      "source": [
        "# Setup sec-edgar\n",
        "from sec_edgar_downloader import Downloader\n",
        "\n",
        "def download_filings(ticker):\n",
        "  \"\"\"\n",
        "  Downloads all available 10-K filings for a given company ticker\n",
        "\n",
        "  Args:\n",
        "      ticker: Ticker of company\n",
        "  \"\"\"\n",
        "\n",
        "  secdl = Downloader(\"DelhiTechnologicalUniversity\",\"keshavnath_me20a9_47@dtu.ac.in\")\n",
        "  # Download specifics\n",
        "  DOCTYPE = \"10-K\"\n",
        "  AFTER = \"1995-01-01\"\n",
        "  BEFORE = \"2024-01-01\"\n",
        "  # Download 10-K filing\n",
        "  try:\n",
        "    secdl.get(DOCTYPE,ticker,after=AFTER,before=BEFORE)\n",
        "  except:\n",
        "    print(f\"Could not find given ticker {ticker}\")\n",
        "  print(\"Downloaded all available filings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-r74xkw8mfm6"
      },
      "outputs": [],
      "source": [
        "# Text cleaning and singling out the actual 10-K filing\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def get_text_content(text_path):\n",
        "  \"\"\"\n",
        "  Extracts textual content from the 10-K HTML file using Beautiful Soup.\n",
        "  1) Open .text file just as a text file even though its actually an XML because its a flawed XML\n",
        "  2) The first contained document is an HTML of the 10-K filing, followed by embedded zip files, images, excel sheets, etc. (not useful)\n",
        "  3) Instead of trying to create an XML Tree, we can simply find the end of the first </DOCUMENT> tag, end the input stream, and open it as an HTML\n",
        "  4) Use bs4 HTML Parser to extract the actual textual content from the HTML (i.e the filing)\n",
        "  \n",
        "  - Future work: Try to use other embedded docs especially the excels and the tables\n",
        "\n",
        "  Args:\n",
        "      text_path: Path to SEC-EDGAR .txt file.\n",
        "\n",
        "  Returns:\n",
        "      String containing the extracted text content of 10-K Filing.\n",
        "  \"\"\"\n",
        "\n",
        "  html_text=\"\"\n",
        "  token = \"</DOCUMENT>\"\n",
        "  # The above variable contains the value < / D O C U M E N T > (w/o spaces) (also mentioned in the docstring: first _ tag) but apparently github doesn't show that token so I'm adding this comment for clarity\n",
        "\n",
        "  try:\n",
        "    with open(text_path, 'r') as f:\n",
        "      # text_content = f.read()\n",
        "      for line in f:\n",
        "        html_text += line\n",
        "        if token in html_text:\n",
        "          break\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {text_path}\")\n",
        "    exit()\n",
        "\n",
        "  soup = BeautifulSoup(html_text, 'html.parser')\n",
        "\n",
        "  # Exclude specific tags and attributes\n",
        "  excluded_tags = ['img', 'script', 'style', 'link', 'object', 'form', 'button', 'td','embed','iframe']\n",
        "  for tag in excluded_tags:\n",
        "    for element in soup.find_all(tag):\n",
        "      element.decompose()  # Remove the entire element\n",
        "\n",
        "  # Extract text from remaining elements\n",
        "  text = ''.join(node.string.strip() for node in soup.find_all(string=True))\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4tDmUA90MftZ"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of year:filing\n",
        "# Used to extract the relevant downloaded filing during RAG\n",
        "import os\n",
        "\n",
        "def create_yearly_dict(ticker):\n",
        "  \"\"\"\n",
        "  Creates a mapping of year: filing path\n",
        "\n",
        "  Args:\n",
        "      Company ticker\n",
        "\n",
        "  Returns:\n",
        "      Dictionary that maps year to filing path\n",
        "  \"\"\"\n",
        "  filings = {}\n",
        "  checkdir = f'./sec-edgar-filings/{ticker}/10-K' #Default save directory template of sec-edgar\n",
        "  for filename in os.listdir(checkdir):\n",
        "    filing_folder = os.path.join(checkdir, filename)\n",
        "    full_path = os.path.join(filing_folder,'full-submission.txt')\n",
        "    year = filename.split('-')[1] # Files are in the format 123xx-YY-456xx\n",
        "    year=int(year)\n",
        "    if (year<=24): # Filenames only contain the last 2 digits of the year so we need to convert them to full years\n",
        "      year+=2000\n",
        "    else:\n",
        "      year+=1900\n",
        "    filings[year]=full_path\n",
        "\n",
        "  return filings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPxnnLCQQoYf"
      },
      "source": [
        "#OpenAI Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "T1MD0GfTYqmO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'openai_api_key' # Replace with API key if want to reproduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-nojtgjwZgwK"
      },
      "outputs": [],
      "source": [
        "# Create chat model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(temperature=0) # Temperature 0 for reproducability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "BXVTSjc0c_en"
      },
      "outputs": [],
      "source": [
        "# In case questions are repeated\n",
        "from langchain.cache import InMemoryCache\n",
        "import langchain\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "langchain.chat_cache = InMemoryCache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ahwUu4qDSSAR"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mL2gbzAVP46g"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0yofDYKSQya"
      },
      "source": [
        "# Document Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "jN0evJ-VK5ox"
      },
      "outputs": [],
      "source": [
        "def extract_years(prompt):\n",
        "  \"\"\"\n",
        "  Extracts all the years from a prompt\n",
        "  Important so that we don't unnecessarily waste credits embedding info we don't need. This is a naive method as it relies on mentions of each year.\n",
        "  - Future work: use better parser (check for words like 'between' to generate list).\n",
        "\n",
        "  Args:\n",
        "      prompt: Input request\n",
        "\n",
        "  Returns:\n",
        "      A list of extracted years in orer of mention\n",
        "  \"\"\"\n",
        "  pattern = r\"\\d{4}\"\n",
        "  extracted_years = re.findall(pattern, prompt)\n",
        "  print(f\"Found years:{extracted_years}\")\n",
        "  return extracted_years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srtuNt-3SObx",
        "outputId": "c19ec2e7-89c7-4985-829a-e20131e4d8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘embeddings’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6WGZkZyrSq_W"
      },
      "outputs": [],
      "source": [
        "# Embedding chunks of all 10-K filings\n",
        "\n",
        "CHUNK_SIZE = 200\n",
        "MAX_DOCS = 15\n",
        "# Total of 3000 tokens given to context, and max 15 different years\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "\n",
        "def embed_filings(ticker,filings_dict,extracted_years):\n",
        "  \"\"\"\n",
        "  Breaks and embeds chunks of 10-K filings per year using OpenAI and ChromaDB\n",
        "\n",
        "  Args:\n",
        "      ticker: Ticker of company\n",
        "      filings_dict: Mapping of filings generated from create_yearly_dict\n",
        "  \"\"\"\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=CHUNK_SIZE) # Recursive to ensure chunk size similarity as break tokens (\\n, \\n\\n, ' ' etc) are inconsistent in HTML\n",
        "\n",
        "  for year in extracted_years:\n",
        "    year = int(year)\n",
        "    filepath = filings_dict[year]\n",
        "    clean_text = get_text_content(filepath)\n",
        "    doc = Document(page_content=clean_text,metadata={\"source\":\"local\"}) # Create langchain document from text\n",
        "\n",
        "    docs = text_splitter.split_documents([doc])\n",
        "    db = Chroma.from_documents(docs, embedding_function, persist_directory=f'./embeddings/{ticker}/{year}')\n",
        "    print(f\"Completed embedding of filings for {ticker}:{year}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No53KtzVTozZ"
      },
      "source": [
        "# Prompting and Document Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "avmThdepYSLJ"
      },
      "outputs": [],
      "source": [
        "# Pydantic output format so we can generate plots automatically\n",
        "# The model can sometimes make mistakes with values in this format (compared w/ raw text output) but its necessary for plots and analysis\n",
        "class Insight(BaseModel):\n",
        "  title: str = Field(description=\"Title of the insight/metric to be reported (e.g. Net Income, Gross Margin as Percentage of Net Sales etc.)\")\n",
        "  datapoints: List[float] = Field(description=\"List of numerical (float) data points and correpsonding years (e.g. [10,20,30]) in ascending order of year\")\n",
        "  years: List[str] = Field(description=\"List of corresponding year for each datapoint (e.g. [2002,2003,2004]) in ascending order\")\n",
        "  unit: str = Field(description=\"Unit of measurement for the values (e.g. Million USD, Percentage, etc.)\")\n",
        "\n",
        "class ModelOutput(BaseModel):\n",
        "\n",
        "  insights: List[Insight] = Field(description=\"List of insights extracted from the given context\")\n",
        "  summary: str = Field(description=\"Textual summary of the insights (e.g. These figures indicate a decline in revenue, gross margin, net income, and earnings per share from 1995 to 1996, along with an increase in restructuring costs. The company faced challenges in maintaining profitability and operational efficiency during this period.)\")\n",
        "\n",
        "  class Config:\n",
        "    arbitrary_types_allowed = True\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=ModelOutput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CtQlZJJSVdXp"
      },
      "outputs": [],
      "source": [
        "def generate_request(ticker,actual_prompt=None):\n",
        "  \"\"\"\n",
        "  Generates the required prompt from an input promp, context, system prompt and output format\n",
        "\n",
        "  Args:\n",
        "      ticker: Company ticker\n",
        "      actual_prompt: Requested prompt (if not given, a default prompt is run)\n",
        "\n",
        "  Returns:\n",
        "      request: Fully-formed ChatPrompt request\n",
        "  \"\"\"\n",
        "\n",
        "  extracted_years = extract_years(actual_prompt)\n",
        "  docs_per_year = MAX_DOCS//len(extracted_years) # Try to fill up 3000 tokens with context\n",
        "\n",
        "  relevant_docs = {}\n",
        "\n",
        "  for extracted_year in extracted_years:\n",
        "    extracted_year=int(extracted_year)\n",
        "    dbl = Chroma(persist_directory=f'./embeddings/{ticker}/{extracted_year}', embedding_function=embedding_function) # Open embeddings of required year\n",
        "    retriever = dbl.as_retriever()\n",
        "\n",
        "    search_kwargs = {\"k\":docs_per_year}\n",
        "    picked = retriever.get_relevant_documents(actual_prompt,search_kwargs=search_kwargs) #Thought of using MultiQueryRetriever but wanted to minimise credit usage\n",
        "\n",
        "    combined_context = ''.join([pick.page_content for pick in picked])\n",
        "    relevant_docs[extracted_year] = combined_context\n",
        "\n",
        "  # Basic Prompt engineering - I didn't really try many other system prompts so this could definitely be approved\n",
        "  system_template = \"You are an expert at carefully reading and analyzing the SEC 10-K filings of a company. You are able to identify key figures about the company from reading portions of these annual reports.\"\n",
        "  system_template+= \" You make sure to report the correct value, sign and unit for each metric/insight.\"\n",
        "  system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "  prompt_template = actual_prompt + \"\\n\"\n",
        "  for extracted_year in relevant_docs.keys():\n",
        "    prompt_template += f\"Content from {extracted_year}:{relevant_docs[extracted_year]}\\n\\n\"\n",
        "\n",
        "  prompt_template+=\"{instructions}\"\n",
        "\n",
        "  human_prompt = HumanMessagePromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  chat_prompt = ChatPromptTemplate.from_messages([system_prompt,\n",
        "                                                  human_prompt]) #Zero shot as of now, but few-shot would certainly make it better\n",
        "  request = chat_prompt.format_prompt(instructions=parser.get_format_instructions())\n",
        "\n",
        "  print(f\"Generated prompts\")\n",
        "\n",
        "  return request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rfUsy5Qnc_h2"
      },
      "outputs": [],
      "source": [
        "def send_request(request):\n",
        "  \"\"\"\n",
        "  Sends request to ChatGPT and gets Pydantic object\n",
        "\n",
        "  Args:\n",
        "      request: Request from prompt function\n",
        "  \"\"\"\n",
        "  result = chat.invoke(request)\n",
        "  result_object = parser.parse(result.content) #Create pydantic object from json\n",
        "\n",
        "  print(f\"LLM called and output generated\")\n",
        "\n",
        "  return result_object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "2ve70BAmdEmK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotter(result_object):\n",
        "  \"\"\"\n",
        "  Creates output plots from result object\n",
        "\n",
        "  Args:\n",
        "      result_object: Pydantic Object generated from send_request\n",
        "\n",
        "  Returns:\n",
        "      Generated plots\n",
        "  \"\"\"\n",
        "\n",
        "  rows = (len(result_object.insights) + 1) // 2  # Adjust rows for a 2-column layout\n",
        "  cols = min(2, len(result_object.insights))  # Adjust columns for a maximum of 2 plots per row\n",
        "  fig, axes = plt.subplots(rows, cols, figsize=(12, 8)) # Easier to pass a single fig to Gradio so add all figs as subplots into a single fig\n",
        "\n",
        "  for i, insight in enumerate(result_object.insights):\n",
        "    ax = axes.flat[i]\n",
        "    title = str(insight.title)\n",
        "    unit = str(insight.unit)\n",
        "    datapoints = insight.datapoints\n",
        "    years = insight.years\n",
        "\n",
        "    ax.bar(years, datapoints) # Bar plot\n",
        "    ax.plot(years, datapoints, marker='o', color='green')  # Line plot\n",
        "    for year, datapoint in zip(years,datapoints):\n",
        "      ax.text(year, datapoint, f\"{datapoint} {unit}\", ha='center', va='bottom', fontsize=8)\n",
        "    ax.set_xlabel(\"Year\")\n",
        "    ax.set_ylabel(f\"{title} ({unit})\")\n",
        "    ax.set_title(f\"{title} in {', '.join(years)}\")\n",
        "    ax.grid(True)\n",
        "\n",
        "  fig.suptitle(f\"Insights ({len(result_object.insights)} total)\", fontsize=14)  # Add overall title\n",
        "  plt.tight_layout()  # Adjust spacing between subplots\n",
        "\n",
        "  print(f\"Completed plot generation\")\n",
        "\n",
        "  # return plt\n",
        "  return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "bj0OlZH7cdBV"
      },
      "outputs": [],
      "source": [
        "saved_tickers = []\n",
        "\n",
        "def pipeline(ticker: str, prompt=None):\n",
        "  \"\"\"\n",
        "  Runs the pipeline from start to finish\n",
        "  - Future work: check for ticker correctness and other error management\n",
        "\n",
        "  Args:\n",
        "    ticker: Company Ticker\n",
        "    prompt: Insight you want to generate (defaults to earning-based insight)\n",
        "\n",
        "  Returns:\n",
        "    (plots: plt object of generated plots,\n",
        "     summary: Textual summary of insights)\n",
        "  \"\"\"\n",
        "\n",
        "  if not prompt:\n",
        "    prompt = \"Can you tell me the differences in company revenue and some other key metrics between the years 1995 and 1996 of this company?\"\n",
        "    # Default insight request if user can't think of one\n",
        "\n",
        "  if (ticker not in saved_tickers):\n",
        "    saved_tickers.append(ticker)\n",
        "    download_filings(ticker) # No need to redownload the filings if we have already used this ticker in the current session\n",
        "\n",
        "  filings_dict = create_yearly_dict(ticker)\n",
        "\n",
        "  embed_filings(ticker,filings_dict,extract_years(prompt))\n",
        "\n",
        "  request = generate_request(ticker,prompt)\n",
        "  result_object = send_request(request)\n",
        "  plots = plotter(result_object)\n",
        "  summary = str(result_object.summary)+\"\\n\"\n",
        "\n",
        "  return plots,summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "T-RlvkYmdwJa",
        "outputId": "c0bf3a6b-cdfc-42ad-84e4-305b0e109edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://09675ed24b574b91ec.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://09675ed24b574b91ec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found years:['1995', '1996']\n",
            "Completed embedding of filings for AAPL:1995\n",
            "Completed embedding of filings for AAPL:1996\n",
            "Found years:['1995', '1996']\n",
            "Generated prompts\n",
            "LLM called and output generated\n",
            "Completed plot generation\n"
          ]
        }
      ],
      "source": [
        "# Basic gradio interface built into the notebook\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "with demo:\n",
        "  gr.Markdown(\"Type a company ticker and request some insight, this will return an insight summary and some plots.\")\n",
        "  gr.Markdown(\"You can leave the prompt blank to obtain a default insight: 'Can you tell me the differences in company revenue and some other key metrics between the years 1995 and 1996 of this company?'\")\n",
        "\n",
        "  gr.Interface(\n",
        "      fn=pipeline,\n",
        "      inputs=[\"text\", \"text\"],\n",
        "      outputs=[gr.Plot(), \"text\"]\n",
        "      )\n",
        "\n",
        "demo.launch(share=True,debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq60Tbkbfzzm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
